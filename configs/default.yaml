# default config.

name: capture_qqtiger_nerf

resume: None

debug:
    debug_mode: False               # support debug like checking grad.
    print_all_grad: False           # print all grad for all param in model. Only in debug mode
    get_progress: False             # get the progress to print during train/val, need save_progress and local_progress

dir:
    expr_dir: None
    data_dir: '/group/30042/leoyluo/Immerse/data'      # It is the main dataset address containing all dataset
    eval_dir: '/group/30042/leoyluo/Immerse/projects/ArcNerf/results/capture_qqtiger_nerf'

model_pt: '/group/30042/leoyluo/Immerse/projects/ArcNerf/experiments/capture_qqtiger_nerf/checkpoints/final.pt.tar'

data:
    video_path: '/group/30042/leoyluo/Immerse/data/Capture/qq_tiger.MOV'
    scene_name: qqtiger
    video_downsample: 1             # fps downsample ratio, default 1
    image_downsample: 1             # image rescale downsample ratio, default 1
    colmap:
        match_type: 'sequential_matcher'        # ['sequential_matcher', 'exhaustive_matcher']
        dense_reconstruct: False                # run for dense reconstruction with point cloud and mesh

gpu_ids: -1                         # -1 is cpu, otherwise use list of gpus

dist:
    local_rank: 0
    rank: 0
    world_size: 1
    distributed_backend: nccl       # nccl/mpi
    random_seed: None               # None will not require reproducibility, run benchmark to be faster
    slurm: False                    # use slurm for management

batch_size: 2                       # actually do not use it in training
n_rays: 4096                        # n_rays for each training batch, does not affect inference/val, capacity by model
worker: 4

optim:
    lr: 5e-4
    optim_type: adam                # [adam, sgd, lbfgs, rmsprop]
    weight_decay: 0.0
    maxiters: 5                     # for lbfgs only
    lr_scheduler:
        type: ExponentialLR         # [MultiStepLR, ExponentialLR, PolyLR, CosineAnnealingLR, WarmUpCosineLR]
        lr_gamma: 0.1               # for ExponentialLR, MultiStepLR, PolyLR
        lr_steps: [200000]          # for ExponentialLR(Use first), MultiStepLR(Use all), PolyLR(Use all)
        tmax: 20                    # for CosineAnnealingLR
        ema_min: 1e-3               # for CosineAnnealingLR
        min_factor: 0.1             # for WarmUpCosineLR

    clip_gradients: 0.0             # grad clipping set for init
    clip_warmup: -1                 # warmup epoches. -1 mean not change after warmup
    clip_gradients_warmup: 0.0      # grad cliping after warmup period. Can be smaller incase explode

progress:
    start_epoch: -1                 # -1: resume; 0: finetune. Only for resume mode
    epoch: 300000                   # Num of epoch for training
    save_time: 1800                 # save model after this time(in second). By default 30min

    epoch_loss: 100                 # Num of epoch to display loss
    iter_loss: 1                    # Num of iteration to display loss
    save_progress: False            # Whether to save progress during training
    epoch_save_progress: 50000      # Num of epoch for saving progress
    iter_save_progress: 1           # Num of iteration for saving progress

    epoch_val: 50000                # Num of epoch for validation. -1 means not validation
    save_progress_val: True         # Whether to save progress during validation
    max_samples_val: 1              # Max num of sample to write into image in valid

    epoch_save_checkpoint: 100000   # Num of epoch save checkpoint
    local_progress: False           # When local progress, will write to local files. Otherwise to tensorboard only

    init_eval: False                # Whether to eval the model at first epoch
    epoch_eval: 100000              # Num of epoch eval model on test set. -1 means no evaluation
    max_samples_eval: 3             # Max num of sample to write into image in eval

dataset:
    train:
        type: Capture
        scene_name: qqtiger
        img_scale: 0.25              # scale image and intrinsic, < 1 means scale_down
        center_by_view_dirs: False   # Whether to center the poses first by view dirs center. Useful when the obj is not in center
        skip: 10                     # skip training samples
        scale_radius: 3.0            # norm the camera within a sphere with such radius(actual radius litter smaller)
        pc_radius: 2.0               # only keep point cloud in a fix radius ball, scale after cam norm
        align_cam: False             # whether to use avg pose to adjust cam rotation. Only for cam that are not horizontally distributed.
        # exchange_coord: [y, z, x]  # exchange and flip coord. Generally not use
        ndc_space: False             # change the rays into ndc_space. You must assure the near is 1.0 before that. You should set near/far as 1 if used ndc_space
        center_pixel: False          # whether to use pixel from (0.5, 1.5,...) instead of (0, 1, 2)
        precache: True               # precache all the rays at once
        device: cpu                  # ['cpu', 'gpu'], gpu brings all tensor to GPU, takes memory but fast in data preparation
        augmentation:                # augmentation is for rays on a single image. Do not change in whole progress
            shuffle: False           # shuffle all rays on a single image
        scheduler:                   # scheduler handles sampling during training in different stage
            ray_sample:
                mode: 'random'       # ['full', 'random'], full takes all rays, random is sample with replacement
                cross_view: False    # used in both mode. If True, each sample takes rays from different image. Else on in one image.
            precrop:                 # crop the image to keep center rays
                ratio: 0.5           # num of ratio keep on each dim
                max_epoch: 500       # max epoch for precropping
#            bkg_color:               # bkg color for training batch, mask must exist
#                color: random        # random background is better for main object optimization. Set [1.0, 1.0, 1.0] for white bkg
#            dynamic_batch_size:      # only use when fg model with volume pruning
#                update_epoch: 16     # freq to update

    val:
        type: Capture
        scene_name: qqtiger
        img_scale: 0.125
        scale_radius: 3.0
        pc_radius: 2.0
        skip: 10
        align_cam: False
        center_pixel: False
        precache: False
        device: cpu
#        augmentation:
#            blend_bkg_color:                       # could be used in lego to force a bkg_color
#                bkg_color: [ 1.0, 1.0, 1.0 ]

    eval:
        eval_batch_size: 1
        eval_max_sample: 3
        type: Capture
        scene_name: qqtiger
        img_scale: 0.25
        scale_radius: 3.0
        pc_radius: 2.0
        skip: 10
        align_cam: False
        center_pixel: False
        precache: True
        device: cpu

inference:
    to_gpu: False                   # If true, directly create rays on gpu, if trained on gpu. Will be faster.
    render:                         # set a list of render camera on sphere
        type: [circle, spiral]      # list of render cam type
        n_cam: [20, 20]             # each render type num of cam
        repeat: [3, 3]              # repeat the render video
        radius: 3.0                 # sphere radius
        u_start: 0.0                # for start pos u in (0~1)
        u_range: [0.0, 0.5]         # u in (0~1) for swing mode only
        v_ratio: 0.0                # for circle path, vertical position
        v_range: [-0.5, 0]          # for spiral path, vertical pos range
        normal: [0.0, 1.0, 0.0]     # for cam path rotation, (0,1,0) is horizontal without rotation
        n_rot: 3                    # n_rot of camera path
        reverse: False              # reverse u range for swing mode only
        fps: 5                      # render video fps
        center_pixel: False         # whether to use pixel from (0.5, 1.5,...) instead of (0, 1, 2)
        # surface_render:           # whether to add surface rendering, good for sdf model
        #     chunk_rays_factor: 1024 # set this to allow model to process more rays in surface_render mode
        #     method: sphere_tracing  # method to find surface pts, ['sphere_tracing', 'secant_root_finding']
        #     n_step: 128           # used for secant_root_finding, split the whole ray into intervals. By default 128
        #     n_iter: 20            # num of iter to run finding algorithm. By default 20
        #     threshold: 0.01       # error bounding to stop the iteration. By default 0.01
        #     level: 0.0            # level set for mesh extracting. For sigma is around 50. For sdf is 0.
        #     grad_dir: ascent      # if 'descent', sigma is larger than level in obj(NeRF), if 'ascent' is smaller(SDF)

    volume:                         # volume params
        origin: [0.0, 0.0, 0.0]     # volume center
        n_grid: 512                 # n_grid is the num of voxel in each dim. For visual set a small num only
        side: 1.5                   # if set, all len at each dim will be side
        # xyz_len: [1.0, 1.0, 1.0]  # xyz-dim length, only when side is None
        level: 50.0                 # level set for mesh extracting. For sigma is around 50. For sdf is 0.
        grad_dir: descent           # if 'descent', sigma is larger than level in obj(NeRF), if 'ascent' is smaller(SDF)
        chunk_pts_factor: 32        # set this to allow model to process more pts in pts_dir forward mode
        render_mesh:                # If True, will render the extracted mesh(in color and geo only)
            backend: pytorch3d      # Select the render backend type. (Open3d, pytorch3d)

model:                              # for more detail on model, visit the `docs/models.md` and `configs/models` for detail
    type: NeRF
#    obj_bound:                       # this helps to bound the obj in a smaller structure and save computation
#        sphere:                      # sphere used for neus/volsdf and some sdf method
#            radius: 2.0
#        volume:                      # volume used for methods like `instant-ngp` and other volume based modeling
#            n_grid: 128
#            side: 2.0
#        epoch_optim: 16              # num of epoch to update the obj_bound, only support volume now.
#        epoch_optim_warmup: 256      # warmup epoch to update all the voxels
#        ray_sampling_acc: True       # For coarse volume sampling. If True, skip the empty cells.
#        ray_sample_fix_step: True    # For coarse volume sampling, use a fix step to sample, which reduce samples num
#        ema_optim_decay: 0.95        # ema to update the opacity. If None, replace directly without ema.
#        opa_thres: 0.01              # opacity to consider as an occupied voxel
#        bkg_color: [1.0, 0.0, 0.2]   # color for invalid rays
#        depth_far: 10.0              # depth value for invalid rays.
#        normal: [0.0, 1.0, 0.0]      # normal for invalid rays
    rays:
        bounding_radius: 3.0          # bounding ray sampling region
        near: 1.0                     # hardcode near zvals
        far: 8.0                      # hardcode far zvals
        n_sample: 64                  # n_sample for zvals sampling
        n_importance: 128             # n_importance for zvals for hierarchical sampling
        inverse_linear: False         # inverse_linear make zvals close to near
        perturb: True                 # perturb zvals interval
        add_inf_z: True               # Add inf zval for ray marching, do not applied for background
        noise_std: 1.0                # noise for sigma in training
        white_bkg: False              # cast the background as white
    chunk_rays: 32768                 # each chunk to process 1024*32(rays), full nerf takes more memory
    chunk_pts: 131072                 # each chunk to progress 4096*32(pts), full nerf takes more memory
    geometry:
        W: 256                        # linear hidden neuron
        D: 8                          # num of linear, not include final
        skips: [4]                    # pos to add input embedding
        encoder:                      # encoder
            type: FreqEmbedder        # type of encoder
            input_dim: 3              # xyz input channel
            n_freqs: 10               # xyz embed freq
        W_feat: 256                   # extra feature output
        geometric_init: False         # whether to use geometric init. need radius_init
    radiance:
        mode: 'vf'                    # view_dir and feature only
        W: 128                        # linear hidden dim
        D: 1                          # num of linear, not include final
        encoder:
            view:
                type: FreqEmbedder    # type of encoder
                input_dim: 3          # view input channel
                n_freqs: 4            # view embed freq
        W_feat_in: 256                # dim for feature from geometry

loss:
    ImgLoss:
        keys: ['rgb_coarse', 'rgb_fine']        # The key in training depends on the model setting
        loss_type: MSE                          # Support `MSE`, `L1`, `Huber`
        weight: 1.0                             # Weight for on loss

metric:
    PSNR:                                       # eval by default only one key `rgb`

train_metric:
    PSNR:
        key: 'rgb_fine'                         # `rgb` for one stage output, `rgb_fine` for two stage fine output
