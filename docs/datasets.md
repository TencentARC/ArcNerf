# base_3d_dataset
Base class for all 3d dataset. Contains image/mask(optional)/camera.
Support precache_ray/norm_cam_pose/rescale_image_pose/get_item in a uniform way.
- Each dataset contains an `identifier` that is a string separating the scene from the same dataset.
(like scan_id, scene_name, etc)

## base_3d_pc_dataset
Based on `base_3d_dataset`, it provides functions mainly on point cloud adjustment.
Points are in world coordinate.
- point_cloud: a dict with 'pts'/'color'/'vis'. Last two are optional.

# Some configs in data processing:
- img_scale: Resize image by this scale. `>1` means larger image.
Will change intrinsic for actual re-projection as well, but not change extrinsic.
- scale_radius: Rescale all camera pose such that cameras are roughly align on the surface of sphere with such radius.
Will not touch intrinsic. If point cloud exists, rescale them by same factor to keep consistency.
    - This actual cam radius will be adjusted by a factor of `1.05` which ensure cam `inside` the sphere, which will be
  good for ray-sphere computation(forbid nan).
- precache: If True, will precache all the rays for all pixels at once.
- pc_radius(base_3d_pc_dataset): Remove point cloud that are outside such absolute radius(all scaled by extra `1.05`).
- align_cam: Sometimes it can be used to align cam in a horizontal way.
- exchange_coord: Flexible to exchange/flip the coord to a standard system.
Done after camera `scale_radius`. The radius is restricted within `scale_radius` range.
## Augmentation:
The augmentation is for all image process in all time.
- n_rays: Sample `n_rays` instead of using all. But calling it every time may sample overlapping rays, not use in train.
- shuffle: shuffle all the rays from the same image together
## rgb and mask
- All color are in `rgb` order and normed by `255` into `0~1` range.
- All masks should be binary masks with `{0,1}` values. Can be [] if not exist.
## cameras
- cameras: a list of `render.camera.PerspectiveCamera`, used for ray generation.
- For every dataset, you should setup cameras by reading their own `c2w` and `intrinsic`
## rays:
- rays are generated by cameras, precache all rays if needed.
- `get_rays` generate rays in `(wh)` flatten order, but if you read by cv2 and reshape, img will be in `(hw)` order.
You need to get_rays by setting `wh_order=False` to change the order.
## point cloud
- pts: `(n_pts, 3)` in world coordinate, `xyz` order
- color: `(n_pts, 3)`, `rgb` order, should be normed into `0~1` range.
- vis: `(n_cam, n_pts)`, visibility of each point in each cam. `{0,1}` values.
- pts is required, color/vis is optional.
## bounds
- a list of bounds in `(2, )` dim representing the near, far zvals.
- If exists, help the ray sampling in modeling progress. Can be [] if not exist.
- For the near/far, you can also set in `cfgs.rays.near/far`, or use ray-sphere intersection
for near/far calculation by setting `cfgs.rays.bounding_radius`.
- Since we use normalized rays for each image, it means that the given the same distance, the ray in center is extended
further than corner ones. It makes the sampling in roughly a sphere, when the cameras aligned on sphere surface.
If you want all the rays extended the same z-distance, you should not normalize the rays, this may be good for the case
of large scene but not object.


# Dataset Class
Below are supported dataset class.
## Capture
This class provides dataset from your capture data.
You need to run colmap to extract corresponding poses and train.
### Video_to_Image
If you capture video, you can follow `scripts/data_process.sh` to use `extract_video` to
extract video into images. Will write data to `cfgs.dir.data_dir/Capture/scene_name`
- video_path: actual video path
- scene_name: specify the scene name. Image will be written to `cfgs.dir.data_dir/Capture/scene_name/images`.
- video_downsample: downsample video frames by such factor. By default `1`.
- image_downsample: downsample each frame by such factor. By default `1`.
We suggest you to run extract/colmap on full image size and length to allow more accurate pose estimation. You can reduce
the image for training by setting config in dataset.
### Colmap Run_poses
You should install [colmap](https://colmap.github.io/) by yourself. We provide python script for processing.
you can follow `scripts/data_process.sh` to use `run_poses` to get colmap with poses and dense reconstruction.
Will write data to `cfgs.dir.data_dir/Capture/scene_name`
- match_type: `sequential_matcher` is good for sequential ordered images.
              `exhaustive_matcher` is good for random ordered image.
- dense_reconstruct: If true, run dense_reconstruct and get dense point cloud and mesh.
### Mask generation
- TODO: We may add it in the future.
### Dataset
Use `Capture` class for this dataset. It is specified by scene_name.
- scene_name: scene_name that is the folder name under `Capture`. Use it to be identifier.
#### Processing
Since we need to rescale the point_cloud and cam so that object(pc) is centered at (0,0,0). If we directly set pc.mean()
as (0,0,0), noise not on object will make the center incorrect. We do the following:
- Use all camera and ray from center image plane to get a closely approximate common view point,
which is close to object center, adjust cam/pc by this offset. This is optional by setting `center_by_view_dirs=True`.
- Norm cam and point by `scale_radius` to make them within a sphere with known range.
- Filter point cloud by `pc_radius` and remove point outside.
- Recenter cam and point by setting the filtered point cloud center as (0,0,0).
- Re-norm cam and point again to make cam on the surface of sphere with `scale_radius` and obj is centered.

We test and show that the method is robust to make the coordinate system such that object is centered at (0,0,0),
cam is on surface with `scale_radius`. Only scale and translation is applied, do not affect the intrinsic.



## Standard benchmarks
### LLFF
This is a forward facing dataset. Not object extraction is performed. Only used to view synthesis.
For fair comparsion, test/val images have not overlapping with train images.
- The camera are aligned flatten. Adjust the poses/bounds by range to avoid large xyz values.
- scene_name: scene_name that is the folder name under `LLFF`. Use it to be identifier.
- NDC: We support NDC Space conversion if you set `ndc_space=True` in dataset.
  - In the original implementation, even they use `ndc_space` rays for sampling, the view_dirs sent to radianceNet is
still in non-ndc space. We don't follow it here but only used `ndc rays_d` as view_dirs. But notice that this affects
the performance, use `non-ndc rays_d` gets better result.

Ref: https://github.com/Fyusion/LLFF & https://github.com/bmild/nerf

### NeRF
Specified by scene_name, read image/camera. Since NeRF split the dataset into train/val/eval, we
load all the camera from all split together, process cameras, and keep the cam for any split. This make the
transformation of camera(like norm_pose) consistent over all split.
- scene_name: scene_name that is the folder name under `NeRF`. Use it to be identifier.
- poses: for the poses, we do transform so that it matches the coord system in our proj.
- images: The image are in `RGBA` channels, needs to blend rgb by alpha.

Ref: https://github.com/bmild/nerf

### DTU
Good for object reconstruction. Specified by scan_id, read image/mask/camera.
- scan_id: int num for item selection. Use it to be identifier.
- eval_max_sample: Select the closest samples for eval. It has overlapping with training view.

Ref: https://github.com/lioryariv/idr/blob/main/code/datasets/scene_dataset.py

### BlendedMVS
Good for object reconstruction. Specified by scene_name, read image/camera.
- scene_name: scene_name that is the folder name under `BlendedMVS`. Use it to be identifier.
- eval_max_sample: Select the closest samples for eval. It has overlapping with training view.
- In some case it uses `align_cam` and `exchange_coord` for changing the coordinate into a standard one.

Ref: https://github.com/YoYo000/BlendedMVS & https://lioryariv.github.io/volsdf/


# Train/Val/Eval/Inference

## Train
Use all images for training. Same resolution as required.

For training, we should read all rays from all images together, shuffle each image pixels(rays), shuffle images,
concat all the image, sample in batch with `n_rays`. Each epoch just trained on batch. When all rays from all images
have been chosen, shuffle again.

For training, `scheduler` will handle special requirement in each shuffle of all rays.


## Val
Use all images for validation, downsampled by 2/4 depends on shape.

Each valid epoch just input one image for rendering, so the batch_size for val is cast to be 1.


## eval
Use several closest camera(to avg_cam) for metric evaluation,

use same resolution(Or scale if image really too large), and use custom cam paths for rendering video

- eval_batch_size: batch size for eval
- eval_max_sample: max num of sample in eval dataset.
only those will be fully rendered can calculate metric.

For some standard benchmark(`NeRF`/`LLFF`), we follow the same split as they used for common comparsion.


## inference
Inference will be performed based on eval dataset params(intrinsic, img shape). If you do not set
the eval dataset, inference will not be performed.

### Render
Controls the params of render novel view(volume rendering), like the camera path. Check `geometry/poses.py` for detail.
  - type: list render cam move type. Support `circle`/`spiral`/`regular`/`swing`.
  - n_cam: list of cam for each type
  - repeat: list of repeat num for each type. In case you don't want to render repeatedly(in `swing`/`circle`).
  - radius: radius of cam path, single value
  - u_start/u_range/v_ratio/v_range/normal/n_rot/reverse: for placing the cameras. Chech `poses` for details.
  - fps: render video fps.

#### surface_render
If you set this, also render the view by finding the surface pts and render. Good for sdf models like Neus and volsdf.
- chunk_rays_factor: In surface_render mode, you can progress more rays in a batch, set a factor to allow large rays batch.
- method: method to find the surface pts. Support `sphere_tracing`(sdf)/and `secant_root_finding`(any).
- n_step/n_iter/threshold: params to find root. Check the `geometry/rays.py` for detail.
- level/grad_dir: Determine the value flow. SDF is 0.0/ascent(inside smaller), density is +level/descent(inside larger).

### Volume
Controls the params of volume estimation and mesh extraction/rendering.

We support extract the mesh from volume field and getting the colors of mesh/verts by normal direction.

- origin/n_grid/side/xlen/ylen: params for volume position and size. Check `geometry/volume.py` for detail.
- level/grad_dir: Determine the value flow. SDF is 0.0/ascent(inside smaller), density is +level/descent(inside larger).
- chunk_pts_factor: In extract_mesh mode, you can progress more pts in a batch, set a factor to allow large pts batch.
- render_mesh: For rasterization of mesh only. You can use `pytorch3d` or `open3d` as backend.
