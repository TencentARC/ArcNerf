{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a5562e6-009e-4fd0-971d-193a301e2428",
   "metadata": {},
   "source": [
    "# Setup your project folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6e1da37-2c3c-4ea5-9034-80b7dd93a370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/group/30042/leoyluo/Immerse/projects/ArcNerf\n"
     ]
    }
   ],
   "source": [
    "%cd /path_to/ArcNerf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebc0e06-86da-4e84-af0f-249ea06b80e1",
   "metadata": {},
   "source": [
    "# Import lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47fb6710-6998-4c38-aedf-86e46e12ec47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from arcnerf.datasets import get_dataset, get_model_feed_in\n",
    "from arcnerf.geometry.poses import generate_cam_pose_on_sphere, invert_poses\n",
    "from arcnerf.models import build_model\n",
    "from arcnerf.render.ray_helper import get_rays\n",
    "from common.utils.cfgs_utils import load_configs\n",
    "from common.utils.img_utils import img_to_uint8\n",
    "from common.utils.logger import Logger\n",
    "from common.utils.model_io import load_model\n",
    "from common.utils.torch_utils import torch_to_np\n",
    "from common.utils.video_utils import write_video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f982d79-219c-4ea7-9b56-8e8abda24aeb",
   "metadata": {},
   "source": [
    "# Specify the model cfgs and model_pt with device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99b5529b-dde6-4139-a46d-d06b346f2d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfgs_file = '/group/30042/leoyluo/Immerse/projects/ArcNerf/configs/inference.yaml'\n",
    "model_pt = '/group/30042/leoyluo/Immerse/projects/ArcNerf/experiments/capture_qqtiger_nerf/checkpoints/final.pt.tar'\n",
    "device = 'gpu'  # 'cpu' or 'gpu'\n",
    "\n",
    "assert os.path.exists(cfgs_file), 'cfgs not exist at {}'.format(cfgs_file)\n",
    "assert os.path.exists(model_pt), 'model file not exist at {}'.format(model_pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9912736d-ddc6-42ff-9429-ea4fe3959ccc",
   "metadata": {},
   "source": [
    "# Set up cfgs, device, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a21f984b-b29b-451f-8a08-227208bfcb4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-20 11:28:20.076 | INFO     | common.utils.logger:add_log:34 - Successfully loaded checkpoint from /group/30042/leoyluo/Immerse/projects/ArcNerf/experiments/Finish/Capture/capture_qqtiger_neus_nerfpprgb/checkpoints/final.pt.tar (at epoch 300000)... Keep Train: True(Start from 300000)\n"
     ]
    }
   ],
   "source": [
    "cfgs = load_configs(cfgs_file)\n",
    "logger = Logger()\n",
    "\n",
    "if torch.cuda.is_available() and device == 'gpu':\n",
    "    torch.cuda.set_device(0)\n",
    "    \n",
    "model = build_model(cfgs, None)\n",
    "model = load_model(logger, model, None, model_pt, cfgs)\n",
    "if device == 'gpu':\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31da54dc-2610-4c70-af42-b40d08293476",
   "metadata": {},
   "source": [
    "# Get intrinsic from data or set manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebfd3af1-6213-4947-872f-07f331172999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get from dataset if you don't know intrinsic\n",
    "# dataset = get_dataset(cfgs.dataset, cfgs.dir.data_dir, mode='eval', logger=logger)\n",
    "# intrinsic = dataset.get_intrinsic(torch_tensor=False)\n",
    "# wh = dataset.get_wh()\n",
    "\n",
    "# set it manually\n",
    "focal = 800.0\n",
    "wh = (540, 960)\n",
    "intrinsic = np.array([[focal, 0, wh[0]/2.0],\n",
    "                      [0, focal, wh[1]/2.0],\n",
    "                      [0, 0, 1]])  # (3, 3)\n",
    "dtype = torch.float32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3d0ebb-79f8-4566-9ef5-078ebc6f5483",
   "metadata": {},
   "source": [
    "# Get visual camera position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "373cf98f-b965-4151-8ccc-4c15a8d6b863",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-20 11:28:23.735 | INFO     | common.utils.logger:add_log:34 - Num of camera in the path 20\n",
      "2022-05-20 11:28:23.736 | INFO     | common.utils.logger:add_log:34 - Getting rays...\n",
      "2022-05-20 11:28:27.678 | INFO     | common.utils.logger:add_log:34 - Each camera has num of rays 518400\n"
     ]
    }
   ],
   "source": [
    "# set up camera path params\n",
    "mode = 'circle'      # render cam type\n",
    "n_cam = 20           # render num of cam\n",
    "repeat = 3         # repeat the render video\n",
    "radius = 2.0         # sphere radius\n",
    "u_start = 0.75       # for start pos u in (0~1)\n",
    "v_ratio = -0.2       # for circle path, vertical position\n",
    "fps = 5              # render video fps\n",
    "\n",
    "# get camera pose\n",
    "c2w = generate_cam_pose_on_sphere(\n",
    "    mode,\n",
    "    radius,\n",
    "    n_cam,\n",
    "    u_start=u_start,\n",
    "    v_ratio=v_ratio,\n",
    "    close=True\n",
    ")\n",
    "logger.add_log('Num of camera in the path {}'.format(n_cam))\n",
    "\n",
    "# get rays\n",
    "logger.add_log('Getting rays...')\n",
    "inputs = []\n",
    "for cam_id in range(n_cam):\n",
    "    t_intrinsic = torch.tensor(intrinsic, dtype=dtype)\n",
    "    t_c2w = torch.tensor(c2w[cam_id], dtype=dtype)\n",
    "    if device == 'gpu':  # create tensor on gpu\n",
    "        t_intrinsic = t_intrinsic.cuda()\n",
    "        t_c2w = t_c2w.cuda()\n",
    "    ray_bundle = get_rays(wh[0], wh[1], t_intrinsic, t_c2w, wh_order=False)  # (HW, 3) * 2\n",
    "    input = {\n",
    "        'c2w': c2w[cam_id],   # (4, 4) np array\n",
    "        'intrinsic': intrinsic,  # (3, 3) np array\n",
    "        'rays_o': ray_bundle[0][None, :],  # (1, HW, 3) torch tensor\n",
    "        'rays_d': ray_bundle[1][None, :],  # (1, HW, 3) torch tensor\n",
    "        'rays_r': ray_bundle[3][None, :]   # (1, HW, 1) torch tensor\n",
    "    }\n",
    "    inputs.append(input)\n",
    "logger.add_log('Each camera has num of rays {}'.format(inputs[0]['rays_o'].shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518192c3-9821-4315-96ff-8b3f15b09326",
   "metadata": {},
   "source": [
    "# Set up surface_render params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "122186f8-70c3-495c-a86b-728df4f13fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_rays_factor = 1            # set this to allow model to process more rays in surface_render mode\n",
    "method = 'secant_root_finding'   # method to find surface pts, ['sphere_tracing', 'secant_root_finding']\n",
    "level = 50.0                     # level set for mesh extracting. For sigma is around 50(No accurate). For sdf is 0.\n",
    "grad_dir = 'descent'             # if 'descent', sigma is larger than level in obj(NeRF), if 'ascent' is smaller(SDF)\n",
    "\n",
    "# model reset chunk_rays\n",
    "origin_chunk_rays = model.get_chunk_rays()\n",
    "model.set_chunk_rays(origin_chunk_rays * chunk_rays_factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2b0372-d544-4fd7-ac10-8a623af64cfb",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Run surface rendering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2fbd5c83-ad17-4a10-afee-0524a66a56d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [01:50<00:00,  5.51s/it]\n",
      "2022-05-20 11:30:17.833 | INFO     | common.utils.logger:add_log:34 -    Surface Render 20 image, each hw(960/540) total time 106.42s\n",
      "2022-05-20 11:30:17.834 | INFO     | common.utils.logger:add_log:34 -     Each image takes time 5.32s\n"
     ]
    }
   ],
   "source": [
    "images = []\n",
    "total_forward_time = 0.0\n",
    "for rays in tqdm(inputs):\n",
    "    # empty cache\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    feed_in, batch_size = get_model_feed_in(rays, device)  # only read rays_o/d here, (1, WH, 3)\n",
    "    assert batch_size == 1, 'Only one image is sent to model at once for inference...'\n",
    "\n",
    "    time0 = time.time()\n",
    "    output = model.surface_render(feed_in, method=method, level=level, grad_dir=grad_dir)  # call surface rendering\n",
    "    total_forward_time += (time.time() - time0)\n",
    "\n",
    "    # get rgb only\n",
    "    rgb = output['rgb']  # (1, HW, 3)\n",
    "    rgb = img_to_uint8(torch_to_np(rgb).copy()).reshape(int(wh[1]), int(wh[0]), 3)  # (H, W, 3), bgr\n",
    "    images.append(rgb)\n",
    "\n",
    "# log time\n",
    "logger.add_log(\n",
    "    '   Surface Render {} image, each hw({}/{}) total time {:.2f}s'.format(\n",
    "        len(images), wh[1], wh[0], total_forward_time\n",
    "    )\n",
    ")\n",
    "logger.add_log('    Each image takes time {:.2f}s'.format(total_forward_time / float(len(images))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08545d9c-7dfa-4816-97f9-3f3f06ba2432",
   "metadata": {},
   "source": [
    "# Write down video, view it locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8631a426-ae9e-4ea6-b784-db7aa9e71595",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-20 11:33:39.304 | INFO     | common.utils.logger:add_log:34 - Write surface render videos to ./results/surface_render.mp4\n"
     ]
    }
   ],
   "source": [
    "file_path = './results/surface_render.mp4'\n",
    "write_video(images * repeat, file_path, fps=fps)\n",
    "logger.add_log('Write surface render videos to {}'.format(file_path))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
